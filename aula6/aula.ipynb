{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preparação do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instalação das bibliotecas (execute apenas se não estiverem instaladas)\n",
    "# !pip install gensim matplotlib scikit-learn pandas numpy spacy plotly\n",
    "\n",
    "# Importações básicas\n",
    "import numpy as np # trabalhar matrizes\n",
    "import pandas as pd # dataframes\n",
    "import matplotlib.pyplot as plt # gráficos\n",
    "from gensim.models import Word2Vec # modelo de vetorização\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE # framework de treinamento\n",
    "import re #regex\n",
    "import nltk # trabalho de frases e palavras\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download de recursos NLTK (se necessário)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3 Preparando um Dataset para Treinamento\n",
    "### 3.1 Carregando Dados de Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto 1: Este filme é incrível, adorei a atuação do protagonista\n",
      "Texto 2: A direção de fotografia é espetacular e o roteiro é envolvente\n",
      "Texto 3: Péssimo filme, desperdicei meu tempo assistindo isso\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo - críticas de filmes (simplificadas)\n",
    "textos = [\n",
    "    \"Este filme é incrível, adorei a atuação do protagonista\",\n",
    "    \"A direção de fotografia é espetacular e o roteiro é envolvente\",\n",
    "    \"Péssimo filme, desperdicei meu tempo assistindo isso\",\n",
    "    \"Os atores são talentosos mas o roteiro é fraco\",\n",
    "    \"Cinematografia belíssima, recomendo assistir no cinema\",\n",
    "    \"Não gostei da história, personagens mal desenvolvidos\",\n",
    "    \"A trilha sonora combina perfeitamente com as cenas\",\n",
    "    \"Filme entediante, previsível do início ao fim\",\n",
    "    \"Os efeitos especiais são impressionantes, tecnologia de ponta\",\n",
    "    \"História emocionante, chorei no final do filme\"\n",
    "]\n",
    "\n",
    "# Verificando os dados\n",
    "for i, texto in enumerate(textos[:3]):  # Mostrando apenas os 3 primeiros\n",
    "    print(f\"Texto {i+1}: {texto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pré-processamento do Texto\n",
    "\n",
    "#### Antes de gerar embeddings, precisamos pré-processar os textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de texto original:\n",
      "Este filme é incrível, adorei a atuação do protagonista\n",
      "\n",
      "Depois do pré-processamento:\n",
      "['filme', 'incrível', 'adorei', 'atuação', 'protagonista']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    texto = re.sub(r'[^a-záàâãéèêíïóôõöúçñ ]', '', texto)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(texto)\n",
    "\n",
    "    # Remover stopwords (opcional, dependendo da aplicação)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento a todos os textos\n",
    "textos_preprocessados = [preprocessar_texto(texto) for texto in textos]\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de texto original:\")\n",
    "print(textos[0])\n",
    "print(\"\\nDepois do pré-processamento:\")\n",
    "print(textos_preprocessados[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementando Word2Vec com Gensim\n",
    "#### 4.1 Treinando um Modelo do Zero\n",
    "#### O que é a dimensionalidade?\n",
    "#### Vamos treinar um modelo Word2Vec simples com nossos dados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parâmetros do modelo\n",
    "vector_size = 100    # Dimensionalidade dos vetores\n",
    "window = 5           # Tamanho da janela de contexto\n",
    "min_count = 1        # Frequência mínima das palavras\n",
    "workers = 4          # Número de threads para treinamento\n",
    "sg = 1               # Modelo Skip-gram (1) ou CBOW (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com 44 palavras no vocabulário\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "model = Word2Vec(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg\n",
    ")\n",
    "\n",
    "print(f\"Modelo treinado com {len(model.wv.key_to_index)} palavras no vocabulário\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Explorando o Modelo Treinado\n",
    "\n",
    "#### Agora que temos nosso modelo, vamos explorar as palavras e seus embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas palavras do vocabulário:\n",
      "['filme', 'história', 'roteiro', 'desperdicei', 'belíssima', 'cinematografia', 'fraco', 'talentosos', 'atores', 'assistindo']\n"
     ]
    }
   ],
   "source": [
    "# Listar algumas palavras do vocabulário\n",
    "palavras = list(model.wv.key_to_index.keys())\n",
    "print(\"Algumas palavras do vocabulário:\")\n",
    "print(palavras[:10])  # Primeiras 10 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filme': 0,\n",
       " 'história': 1,\n",
       " 'roteiro': 2,\n",
       " 'desperdicei': 3,\n",
       " 'belíssima': 4,\n",
       " 'cinematografia': 5,\n",
       " 'fraco': 6,\n",
       " 'talentosos': 7,\n",
       " 'atores': 8,\n",
       " 'assistindo': 9,\n",
       " 'tempo': 10,\n",
       " 'péssimo': 11,\n",
       " 'assistir': 12,\n",
       " 'envolvente': 13,\n",
       " 'espetacular': 14,\n",
       " 'fotografia': 15,\n",
       " 'direção': 16,\n",
       " 'protagonista': 17,\n",
       " 'atuação': 18,\n",
       " 'adorei': 19,\n",
       " 'incrível': 20,\n",
       " 'recomendo': 21,\n",
       " 'final': 22,\n",
       " 'chorei': 23,\n",
       " 'previsível': 24,\n",
       " 'emocionante': 25,\n",
       " 'ponta': 26,\n",
       " 'tecnologia': 27,\n",
       " 'impressionantes': 28,\n",
       " 'especiais': 29,\n",
       " 'efeitos': 30,\n",
       " 'fim': 31,\n",
       " 'início': 32,\n",
       " 'entediante': 33,\n",
       " 'gostei': 34,\n",
       " 'cenas': 35,\n",
       " 'perfeitamente': 36,\n",
       " 'combina': 37,\n",
       " 'sonora': 38,\n",
       " 'trilha': 39,\n",
       " 'desenvolvidos': 40,\n",
       " 'mal': 41,\n",
       " 'personagens': 42,\n",
       " 'cinema': 43}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3898495e-04  2.3754722e-04  5.1084701e-03  9.0095662e-03\n",
      " -9.3060182e-03 -7.1179080e-03  6.4590140e-03  8.9732325e-03\n",
      " -5.0089178e-03 -3.7637462e-03  7.3839580e-03 -1.5429476e-03\n",
      " -4.5339474e-03  6.5549053e-03 -4.8634596e-03 -1.8150365e-03\n",
      "  2.8793407e-03  9.9878211e-04 -8.2852831e-03 -9.4514769e-03\n",
      "  7.3103900e-03  5.0675846e-03  6.7592384e-03  7.6005177e-04\n",
      "  6.3530109e-03 -3.4019963e-03 -9.5264451e-04  5.7722717e-03\n",
      " -7.5249239e-03 -3.9323601e-03 -7.5148693e-03 -9.2767383e-04\n",
      "  9.5453048e-03 -7.3271766e-03 -2.3372965e-03 -1.9302175e-03\n",
      "  8.0870856e-03 -5.9312731e-03  4.2869444e-05 -4.7512343e-03\n",
      " -9.6031642e-03  5.0065401e-03 -8.7594036e-03 -4.3953191e-03\n",
      " -3.3294396e-05 -2.9962539e-04 -7.6586660e-03  9.6185729e-03\n",
      "  4.9843234e-03  9.2339637e-03 -8.1531862e-03  4.4953576e-03\n",
      " -4.1356767e-03  8.2382484e-04  8.4956065e-03 -4.4643418e-03\n",
      "  4.5180386e-03 -6.7829816e-03 -3.5452251e-03  9.4016502e-03\n",
      " -1.5767787e-03  3.2035878e-04 -4.1417871e-03 -7.6862508e-03\n",
      " -1.5016500e-03  2.4671410e-03 -8.8506279e-04  5.5344566e-03\n",
      " -2.7436947e-03  2.2614619e-03  5.4569058e-03  8.3484771e-03\n",
      " -1.4522823e-03 -9.2028687e-03  4.3761330e-03  5.7426962e-04\n",
      "  7.4392390e-03 -8.1702234e-04 -2.6337255e-03 -8.7473402e-03\n",
      " -8.5956510e-04  2.8262755e-03  5.4018986e-03  7.0487335e-03\n",
      " -5.7079527e-03  1.8547688e-03  6.0868002e-03 -4.8000212e-03\n",
      " -3.1051093e-03  6.8035545e-03  1.6332354e-03  1.9418275e-04\n",
      "  3.4722297e-03  2.1274926e-04  9.6273180e-03  5.0582150e-03\n",
      " -8.9172907e-03 -7.0370454e-03  8.9642097e-04  6.3917381e-03]\n"
     ]
    }
   ],
   "source": [
    "if 'cinema' in model.wv:\n",
    "    vector_filme = model.wv['filme']\n",
    "    print(vector_filme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vetor da palavra 'filme' (primeiras 10 dimensões):\n",
      "[-0.00053898  0.00023755  0.00510847  0.00900957 -0.00930602 -0.00711791\n",
      "  0.00645901  0.00897323 -0.00500892 -0.00376375]\n",
      "Dimensionalidade do vetor: 100\n"
     ]
    }
   ],
   "source": [
    "# Verificar o vetor de uma palavra específica\n",
    "if 'filme' in model.wv:\n",
    "    vetor_filme = model.wv['filme']\n",
    "    print(f\"\\nVetor da palavra 'filme' (primeiras 10 dimensões):\")\n",
    "    print(vetor_filme[:10])\n",
    "    print(f\"Dimensionalidade do vetor: {len(vetor_filme)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Palavras Mais Similares\n",
    "#### Uma das funcionalidades mais interessantes dos word embeddings é encontrar palavras similares:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras mais similares a 'filme':\n",
      "direção: 0.2189\n",
      "assistindo: 0.2168\n",
      "impressionantes: 0.1955\n",
      "cinema: 0.1694\n",
      "efeitos: 0.1520\n"
     ]
    }
   ],
   "source": [
    "# Encontrar palavras mais similares a 'filme'\n",
    "if 'filme' in model.wv:\n",
    "    similares = model.wv.most_similar('filme', topn=5)\n",
    "    print(\"\\nPalavras mais similares a 'filme':\")\n",
    "    for palavra, similaridade in similares:\n",
    "        print(f\"{palavra}: {similaridade:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Salvando e Carregando o Modelo\n",
    "\n",
    "#### Para reutilizar nosso modelo futuramente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cuiabano.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuiaba = Word2Vec.load(\"cuiabano.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7059b0a5b200>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuiaba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Usando Modelos Pré-treinados\n",
    "\n",
    "#### Treinar seu próprio modelo é útil, mas em muitos casos é mais prático usar modelos pré-treinados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50']\n"
     ]
    }
   ],
   "source": [
    "print([nome for nome in list(api.info()['models'].keys())[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "modelo_pretreinado = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras mais similares a 'computer':\n",
      "computers: 0.8752\n",
      "software: 0.8373\n",
      "technology: 0.7642\n",
      "pc: 0.7366\n",
      "hardware: 0.7290\n"
     ]
    }
   ],
   "source": [
    "# Verificar palavras similares usando modelo pré-treinado\n",
    "if 'computer' in modelo_pretreinado:\n",
    "    similares = modelo_pretreinado.most_similar('computer', topn=5)\n",
    "    print(\"\\nPalavras mais similares a 'computer':\")\n",
    "    for palavra, similaridade in similares:\n",
    "        print(f\"{palavra}: {similaridade:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogia: rei - homem + mulher =\n",
      "queen: 0.7699\n",
      "monarch: 0.6843\n",
      "throne: 0.6756\n"
     ]
    }
   ],
   "source": [
    "# Usando modelo pré-treinado para analogias (se disponível)\n",
    "try:\n",
    "    # Famosa analogia: rei - homem + mulher = rainha\n",
    "    if all(word in modelo_pretreinado for word in ['king', 'man', 'woman']):\n",
    "        resultado = modelo_pretreinado.most_similar(\n",
    "            positive=['king', 'woman'],\n",
    "            negative=['man'],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nAnalogia: rei - homem + mulher =\")\n",
    "        for palavra, similaridade in resultado:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except:\n",
    "    print(\"Não foi possível realizar a operação de analogia com o modelo disponível\")\n",
    "\n",
    "# Podemos tentar outras analogias com nosso modelo treinado\n",
    "# Por exemplo: bom - positivo + negativo = ruim\n",
    "# (Dependendo do tamanho do corpus, pode não funcionar bem para modelos pequenos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Calculando Similaridade entre Palavras\n",
    "\n",
    "### Vamos explorar como calcular a similaridade entre palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similaridade entre pares de palavras:\n",
      "filme - cinema: 0.1693534404039383\n",
      "bom - ruim: Uma ou ambas as palavras não estão no vocabulário\n",
      "ator - teleespectador: Uma ou ambas as palavras não estão no vocabulário\n",
      "filme - protagonista: -0.11359719187021255\n"
     ]
    }
   ],
   "source": [
    "# Função para calcular similaridade entre pares de palavras\n",
    "def calcular_similaridade(modelo, pares_palavras):\n",
    "    resultados = []\n",
    "    for par in pares_palavras:\n",
    "        palavra1, palavra2 = par\n",
    "        if palavra1 in modelo.wv and palavra2 in modelo.wv:\n",
    "            similaridade = modelo.wv.similarity(palavra1, palavra2)\n",
    "            resultados.append((par, similaridade))\n",
    "        else:\n",
    "            resultados.append((par, \"Uma ou ambas as palavras não estão no vocabulário\"))\n",
    "    return resultados\n",
    "\n",
    "# Pares de palavras para testar\n",
    "pares = [\n",
    "    ('filme', 'cinema'),\n",
    "    ('bom', 'ruim'),\n",
    "    ('ator', 'atuação'),\n",
    "    ('filme', 'protagonista')\n",
    "]\n",
    "\n",
    "# Calcular similaridades\n",
    "similaridades = calcular_similaridade(model, pares)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nSimilaridade entre pares de palavras:\")\n",
    "for (palavra1, palavra2), similaridade in similaridades:\n",
    "    if isinstance(similaridade, float):\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade:.4f}\")\n",
    "    else:\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Aplicação Prática: Classificação de Sentimentos com Embeddings\n",
    "\n",
    "### Vamos ver como os embeddings podem melhorar um classificador de sentimentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados com TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "\n",
      "Resultados com Word Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dados rotulados para exemplo\n",
    "textos_rotulados = textos  # Usando os mesmos textos de antes\n",
    "sentimentos = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1: positivo, 0: negativo\n",
    "\n",
    "# Função para gerar vetores de documento usando embeddings\n",
    "def texto_para_vetor(texto, modelo):\n",
    "    \"\"\"Converte um texto em um vetor médio dos embeddings de suas palavras\"\"\"\n",
    "    palavras = preprocessar_texto(texto)\n",
    "    # Filtrar palavras que estão no vocabulário do modelo\n",
    "    palavras_no_vocab = [p for p in palavras if p in modelo.wv]\n",
    "    if not palavras_no_vocab:\n",
    "        # Se nenhuma palavra estiver no vocabulário, retorna vetor de zeros\n",
    "        return np.zeros(modelo.vector_size)\n",
    "    # Calcular a média dos vetores das palavras\n",
    "    vetores = [modelo.wv[palavra] for palavra in palavras_no_vocab]\n",
    "    return np.mean(vetores, axis=0)\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    textos_rotulados, sentimentos, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Abordagem com TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "clf_tfidf = LogisticRegression(random_state=42)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# 2. Abordagem com Word Embeddings\n",
    "X_train_emb = np.array([texto_para_vetor(texto, model) for texto in X_train])\n",
    "X_test_emb = np.array([texto_para_vetor(texto, model) for texto in X_test])\n",
    "\n",
    "clf_emb = LogisticRegression(random_state=42)\n",
    "clf_emb.fit(X_train_emb, y_train)\n",
    "y_pred_emb = clf_emb.predict(X_test_emb)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\nResultados com TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "print(\"\\nResultados com Word Embeddings:\")\n",
    "print(classification_report(y_test, y_pred_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
